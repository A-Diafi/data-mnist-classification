{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üéØ <b><u>Exercise objectives</u></b>\n",
    "- Understand the *MNIST* dataset \n",
    "- Design your first **Convolutional Neural Network** (*CNN*) and answer questions such as:\n",
    "    - what are *Convolutional Layers*? \n",
    "    - how many *parameters* are involved in such a layer?\n",
    "- Train this CNN on images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üöÄ <b><u>Let's get started!</u></b>\n",
    "\n",
    "Imagine that we are  back in time into the 90's.\n",
    "You work at a *Post Office* and you have to deal with an enormous amount of letters on a daily basis. How could you automate the process of reading the ZIP Codes, which are a combination of 5 handwritten digits? \n",
    "\n",
    "This task, called the **Handwriting Recognition**, used to be a very complex problem back in those days. It was solved by *Bell Labs* (among others) where one of the Deep Learning gurus, [*Yann Le Cun*](https://en.wikipedia.org/wiki/Yann_LeCun), used to work.\n",
    "\n",
    "From [Wikipedia](https://en.wikipedia.org/wiki/Handwriting_recognition):\n",
    "\n",
    "> Handwriting recognition (HWR), also known as Handwritten Text Recognition (HTR), is the ability of a computer to receive and interpret intelligible handwritten input from sources such as paper documents, photographs, touch-screens and other devices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Number recognition](recognition.gif)\n",
    "\n",
    "*Note: The animation above is just here to help you visualize what happens with the different images: <br/> $\\rightarrow$ For each image, once the CNN is trained, it will predict what digit is written. The inputs are the different digits and not one animation/video!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ü§î <b><u>How does this CNN work ?</u></b>\n",
    "\n",
    "- *Inputs*: Images (_each image shows a handwritten digit_)\n",
    "- *Target*: For each image, you want your CNN model to predict the correct digit (between 0 and 9)\n",
    "    - It is a **multi-class classification** task (more precisely a 10-class classification task since there are 10 different digits).\n",
    "\n",
    "üî¢ To improve the capacity of the Convolutional Neural Network to read these numbers, we need to feed it with many images representing handwritten digits. This is why the üìö [**MNIST dataset**](http://yann.lecun.com/exdb/mnist/) *(Mixed National Institute of Standards and Technology)* was created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) The `MNIST` Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìö Tensorflow/Keras offers multiple [**datasets**](https://www.tensorflow.org/api_docs/python/tf/keras/datasets) to play with:\n",
    "- *Vectors*: `boston_housing` (regression)\n",
    "- *Images* : `mnist`, `fashion_mnist`, `cifar10`, `cifar100` (classification)\n",
    "- *Texts*: `imbd`, `reuters` (classification/sentiment analysis)\n",
    "\n",
    "\n",
    "üíæ You can **load the MNIST dataset** with the following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(((60000, 28, 28), (60000,)), ((10000, 28, 28), (10000,)))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras import datasets\n",
    "\n",
    "\n",
    "# Loading the MNIST Dataset...\n",
    "(X_train, y_train), (X_test, y_test) = datasets.mnist.load_data(path=\"mnist.npz\")\n",
    "\n",
    "# The train set contains 60 000 images, each of them of size 28x28\n",
    "# The test set contains 10 000 images, each of them of size 28x28\n",
    "(X_train.shape, y_train.shape), (X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1.1) Exploring the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question: Let's have look at some handwritten digits of this MNIST dataset.** ‚ùì\n",
    "\n",
    "üñ® Print some images from the *train set*.\n",
    "\n",
    "<details>\n",
    "    <summary><i>Hints</i></summary>\n",
    "\n",
    "üí°*Hint*: use the `imshow` function from `matplotlib` with `cmap = \"gray\"`\n",
    "\n",
    "ü§® Note: if you don't specify this *cmap* argument, the weirdly displayed colors are just Matplotlib defaults...\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "plt.imshow(X_train[0], cmap='gray')\n",
    "plt.title(f'MNIST Image (Label: {y_train[0]})')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1.2) Image Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùóÔ∏è **Neural Networks converge faster when the input data is somehow normalized** ‚ùóÔ∏è\n",
    "\n",
    "üë©üèª‚Äçüè´ How do we proceed for Convolutional Neural Networks ?\n",
    "* The `RBG` intensities are coded between 0 and 255. \n",
    "* We can simply divide the input data by the maximal value 255 to have all the pixels' intensities between 0 and 1 üòâ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question ‚ùì As a first preprocessing step, please normalize your data.** \n",
    "\n",
    "Don't forget to do it both on your train data and your test data.\n",
    "\n",
    "(*Note: you can also center your data, by subtracting 0.5 from all the values, but it is not mandatory*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Normalize the pixel values of the images\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1.3) Inputs' dimensionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëÜ Remember that you have 60,000 training images and 10,000 test images, each of size $(28, 28)$. However...\n",
    "\n",
    "> ‚ùóÔ∏è  **`Convolutional Neural Network models need to be fed with images whose last dimension is the number of channels`.**  \n",
    "\n",
    "> üßëüèª‚Äçüè´ The shape of tensors fed into ***ConvNets*** is the following: `(NUMBER_OF_IMAGES, HEIGHT, WIDTH, CHANNELS)`\n",
    "\n",
    "üïµüèªThis last dimension is clearly missing here. Can you guess the reason why?\n",
    "<br>\n",
    "<details>\n",
    "    <summary><i>Answer<i></summary>\n",
    "        \n",
    "* All these $60000$ $ (28 \\times 28) $ pictures are black-and-white $ \\implies $ Each pixel lives on a spectrum from full black (0) to full white (1).\n",
    "        \n",
    "    * Theoretically, you don't need to know the number of channels for a black-and-white picture since there is only 1 channel (the \"whiteness\" of \"blackness\" of a pixel). However, it is still mandatory for the model to have this number of channels explicitly stated.\n",
    "        \n",
    "    * In comparison, colored pictures need multiple channels:\n",
    "        - the RGB system with 3 channels (<b><span style=\"color:red\">Red</span> <span style=\"color:green\">Green</span> <span style=\"color:blue\">Blue</span></b>)\n",
    "        - the CYMK system  with 4 channels (<b><span style=\"color:cyan\">Cyan</span> <span style=\"color:magenta\">Magenta</span> <span style=\"color:yellow\">Yellow</span> <span style=\"color:black\">Black</span></b>)\n",
    "        \n",
    "        \n",
    "</details>        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question: expanding dimensions** ‚ùì\n",
    "\n",
    "* Use the **`expand_dims`** to add one dimension at the end of the training data and test data.\n",
    "\n",
    "* Then, print the shapes of `X_train` and `X_test`. They should respectively be equal to $(60000, 28, 28, 1)$ and $(10000, 28, 28, 1)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.backend import expand_dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28, 1)\n",
      "(10000, 28, 28, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-07 16:23:31.944925: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 376320000 exceeds 10% of free system memory.\n"
     ]
    }
   ],
   "source": [
    "pass  # YOUR CODE HERE\n",
    "\n",
    "X_train = expand_dims(X_train, axis = -1)\n",
    "\n",
    "X_test = expand_dims(X_test, axis = -1)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1.4) Target encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One more thing to do for a multiclass classification task in Deep Leaning:\n",
    "\n",
    "üëâ _\"one-hot-encode\" the categories*_\n",
    "\n",
    "‚ùì **Question: encoding the labels** ‚ùì \n",
    "\n",
    "* Use **`to_categorical`** to transform your labels. \n",
    "* Store the results into two variables that you can call **`y_train_cat`** and **`y_test_cat`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "y_train_cat = to_categorical(y_train, num_classes = 10, dtype = 'float32')\n",
    "y_test_cat = to_categorical(y_test, num_classes = 10, dtype = 'float32')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick check that you correctly used to_categorical\n",
    "assert(y_train_cat.shape == (60000,10))\n",
    "assert(y_test_cat.shape == (10000,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is now ready to be used. ‚úÖ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) The Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2.1) Architecture and compilation of a CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "‚ùì **Question: CNN Architecture and compilation** ‚ùì\n",
    "\n",
    "Now, let's build a <u>Convolutional Neural Network</u> that has: \n",
    "\n",
    "\n",
    "- a `Conv2D` layer with 8 filters, each of size $(4, 4)$, an input shape suitable for your task, the `relu` activation function, and `padding='same'`\n",
    "- a `MaxPool2D` layer with a `pool_size` equal to $(2, 2)$\n",
    "- a second `Conv2D` layer with 16 filters, each of size $(3, 3)$, and the `relu` activation function\n",
    "- a second `MaxPool2D` layer with a `pool_size` equal to $(2, 2)$\n",
    "\n",
    "\n",
    "- a `Flatten` layer\n",
    "- a first `Dense` layer with 10 neurons and the `relu` activation function\n",
    "- a last (predictive) layer that is suited for your task\n",
    "\n",
    "In the function that initializes this model, do not forget to include the <u>compilation of the model</u>, which:\n",
    "* optimizes the `categorical_crossentropy` loss function,\n",
    "* with the `adam` optimizer, \n",
    "* and the `accuracy` as the metrics\n",
    "\n",
    "(*Note: you could add more classification metrics if you want but the dataset is well balanced!*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "\n",
    "\n",
    "def initialize_model():\n",
    "\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Conv2D(8, (4,4), input_shape=(28, 28, 1), padding='same', activation=\"relu\"))\n",
    "    model.add(layers.MaxPool2D(pool_size=(2,2)))\n",
    "    model.add(layers.Conv2D(16, (3,3), activation=\"relu\"))\n",
    "    model.add(layers.MaxPool2D(pool_size=(2,2))) \n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(10, activation='relu')) # intermediate layer\n",
    "    model.add(layers.Dense(10, activation='softmax'))\n",
    "    model.summary()\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', # No need to OHE target\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question: number of trainable parameters in a convolutional layer** ‚ùì \n",
    "\n",
    "How many trainable parameters are there in your model?\n",
    "1. Compute them with ***model.summary( )*** first\n",
    "2. Recompute them manually to make sure you properly understood ***what influences the number of weights in a CNN***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_2 (Conv2D)           (None, 28, 28, 8)         136       \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 14, 14, 8)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 12, 12, 16)        1168      \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 6, 6, 16)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 576)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                5770      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 10)                110       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7,184\n",
      "Trainable params: 7,184\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "model.summary()\n",
    "\n",
    "# manually calculating parameters\n",
    "# (8 * (4 * 4) + 8) first layer \n",
    "# (16 * 8 * (3 * 3) + 16) second layer \n",
    "# (576 * 10 + 10 ) 3rd layer after flatenning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2.2) Training a CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question: training a CNN** ‚ùì \n",
    "\n",
    "Initialize your model and fit it on the train data. \n",
    "- Do not forget to use a **Validation Set/Split** and an **Early Stopping criterion**. \n",
    "- Limit yourself to 5 epochs max in this challenge, just to save some precious time for the more advanced challenges!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f69d15dd210>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "es = EarlyStopping(patience=2, restore_best_weights=True)\n",
    "\n",
    "model.fit(X_train, y_train_cat,\n",
    "          batch_size=16, # Batch size -too small--> no generalization\n",
    "          epochs=5,    #            -too large--> slow computations\n",
    "          validation_split=0.3,\n",
    "          callbacks=[es],\n",
    "          verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question: How many iterations does the CNN perform per epoch** ‚ùì\n",
    "\n",
    "_Note: it has nothing to do with the fact that this is a CNN. This is related to the concept of forward/backward propagation already covered during the previous lecture on optimizers, fitting, and losses üòâ_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "source": [
    "> YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary><i>Answer</i></summary>\n",
    "\n",
    "With `verbose = 1` when fitting your model, you have access to crucial information about your training procedure.\n",
    "    \n",
    "Remember that we've just trained our CNN model on $60000$ training images\n",
    "\n",
    "If the chosen batch size is 32: \n",
    "\n",
    "* For each epoch, we have $ \\large \\lceil \\frac{60000}{32} \\rceil = 1875$ minibatches <br/>\n",
    "* The _validation_split_ is equal to $0.3$ - which means that within one single epoch, there are:\n",
    "    * $ \\lceil 1875 \\times (1 - 0.3) \\rceil = \\lceil 1312.5 \\rceil = 1313$ batches are used to compute the `train_loss` \n",
    "    * $ 1875 - 1312 = 562 $ batches are used to compute the `val_loss`\n",
    "    * **The parameters are updated 1313 times per epoch** as there are 1313 forward/backward propagations per epoch !!!\n",
    "\n",
    "\n",
    "üëâ With so many updates of the weights within one epoch, you can understand why this CNN model converges even with a limited number of epochs.\n",
    "\n",
    "</details>    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2.3) Evaluating its performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question: Evaluating your CNN** ‚ùì \n",
    "\n",
    "What is your **`accuracy on the test set?`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 5ms/step - loss: 0.0573 - accuracy: 0.9810\n",
      "[0.05730589106678963, 0.9810000061988831]\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "print(model.evaluate(X_test, y_test_cat, verbose=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üéâ You should already be impressed by your CNN skills! Reaching over 95% accuracy!\n",
    "\n",
    "üî• You solved what was a very hard problem 30 years ago with your own CNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üèÅ **Congratulations!**\n",
    "\n",
    "üíæ Don't forget to `git add/commit/push` your notebook...\n",
    "\n",
    "üöÄ ... and move on to the next challenge!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
